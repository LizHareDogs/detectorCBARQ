---
title: "Factor Analysis for Detector Dog C-BARQ for Combined Phase 1 and Phase 2 Data"
subtitle: "Version 2.0 of Combined Factor Analysis (No Chasing Data) Using Items with Communality greater than or equal to0.40 from analysis 1.0"
author:
    name: Liz Hare, PhD
    email: lizhare@gmail.com 
date: "`r Sys.Date()`"
output:
    html_document:
        toc: true
        number_sections: true
---

``` {r, setup, echo=FALSE}
library(knitr)
library(kableExtra)
library(gtsummary)
library(psych)
library(lavaan)
library(EFAtools)
library(missMDA)
library(gdata)

load("data/ddCBARQnumeric.Rda")
load("data/ddCBARQordered.Rda")
descItems <- read.csv("data/cbarqItemDescriptions.csv")

```


``` {r, removeLowCommunalityItems, include=FALSE}
files <- list.files(path = "~/data/dhs/detectorCBARQ/data",
                    pattern = "lc",
                    full.names = TRUE)
for (i in files) {
    load(i)
    }

```





# Context  

## Overall Factor Analysis Steps  

   1. Are the data suitable for factor analysis?  
      - Bartlett's Sphericity Test 
	  Tests whether correlations are different than zero across all the data. 
	  If they are close to zero, there owuld be no common factors to find
	  - Kaiser-Meyer-Olkin Criterion
	  Test sampling adequacy, measures the degree to wich each variable predicts all other variables in the data set.  
	  
   2. Imputation of Missing Variables  
      - Generate categorical values for missing data using Multiple Correspondence Analysis/Missing Fuzzy Average Method  
	  
	  
   3. Calculate between-item Correlations  
      - the polychoric (for orderd categorical variables) and regular Pearson correlation are calculated for later use in factor analysis and for checking other aspects of the factor analysis. The polychoric correlation is the input to the actual factor analysis.  
	  
   4. Estimate Number of Factors  
      - Parallel analysis  
      - Minimim Average Partial (MAP)  
      
   5. Run factor analysis for a range of possible number of factors  
      - produces  
         - measures of model fit  
         - communalities (how much each item covaries with the other items)  
         - loadings (correlation between item and factor(s))  

   6. Evaluate Reliability of Model..
      - alpha   
      - omega  
	  
   7. Select best model (number of factors) given model fit statistics and judgement based on understanding of behavior.

## Context Text Description

All data were used in the factorAnalysis1.1.html document you already received.  

A series of factoro analyses were run on all items in 
the file factorAnalysis1.1.html. THe literature suggests 
doing this so you can compare both statistical measures of 
how well it fits, and also how logical the grouping of factors
is according to knowledge of the subject. 

This set of analyses showed that some items had really low communalities,
meaning that they didn't covary much with any of the other items. This means they
will not be robust parts of any factor, and they recommend that you remove 
these from the analysis.

Models with different numbers of factors had different numbers of items
with communalities less than 0.4. So a new data set was made for each 
number of factors, with only the items  with
communalities of 0.4 or larger.

Because each number of factors require a different data set 
(different numbers of items), all the steps had to be rerun sstarting with
imputation. Those are the models included in this document.

	  
# Factor Analyses

Testing analyses from 11 - 18 factors.


``` {r, runFAs, results="asis", echo=FALSE}
src <- lapply(c(11:18), function(nfact) {
    knit_expand(file = "fa2.0.Rmd")
    })

res = knitr::knit_child(text = unlist(src), quiet = TRUE)
cat(res, sep = '\n')

```
