---
title: "Factor Analysis for Detector Dog C-BARQ for Combined Phase 1 and Phase 2 Data"
subtitle: "Version 3.0 with 15 factors"
author:
    name: Liz Hare, PhD
    email: lizhare@gmail.com 
date: "`r Sys.Date()`"
output:
    html_document:
        toc: true
        number_sections: true
---

``` {r, setup, echo=FALSE}
library(knitr)
library(tidyr)
library(dplyr)
library(kableExtra)
library(gtsummary)
library(psych)
library(EFAtools)
library(missMDA)
library(readr)
library(gdata)
library(writexl)

load("data/ddCBARQnumeric.Rda")
load("data/ddCBARQordered.Rda")
descItems <- read.csv("data/cbarqItemDescriptions.csv")


```


``` {r, removeItems, include=FALSE}
drop15 <- c("TRAIN05", "TRAIN06", "TRAIN08",
                        "AGG11", "AGG12", "AGG13", "AGG17",
                        "FEAR22", "FEAR25",
                        "ATT40", "ATT42",
                        "IMP49", "MISC50", "MISC51", "MISC57", "MISC58",
                        "MISC63", "MISC68", "MISC69", "MISC70", "MISC71")
ddNum15 <- ddItems[ , -which(names(ddItems) %in% drop15)]
ddOrd15 <- ddItemsOrdered[ , -which(names(ddItemsOrdered) %in% drop15)]

```

	  
# 15-Factor Model with Item MISC69 removed

Item MISC69 is "Chases/follows shadows, light spots, etc."  We decided to remove it because it sorted with fear of underfootings


## Imputation

Categorical missing values using multiple Correspondence Analysis (also called Missing Fuzzy Average method)
Josseet al (201
0)


``` {r, imputation,  results="asis", echo=FALSE}
imputationResults15 <- imputeMCA(ddOrd15)
postImputedFactors15 <- imputationResults15$completeObs
imputedChar15 <- lapply(postImputedFactors15, as.character)
imputedNumeric15 <- lapply(imputedChar15, as.numeric)
imputedNumericDF15 <- data.frame(imputedNumeric15)
save(imputedNumericDF15, file="data/3.0/imputedNumericDF3.0")
write.csv(imputedNumericDF15, file="data/3.0/imputedNumeric15.csv", quote = FALSE, row.names=FALSE)

```

``` {r, cors, results="asis", echo=FALSE}
### calculate polychoric correlation
preImputedCor15 <-polychoric(ddNum15,
                             smooth = TRUE,
                             correct = 0.01)
preImputedCor15 <- round(preImputedCor15$rho, 3)
write.csv(preImputedCor15, file = "data/3.0/preImputedCor15.csv",
          quote=FALSE, row.names=FALSE)


### preImputation Bartlett
preBartlett15 <- cortest.bartlett(preImputedCor15,
                                  n = nrow(ddOrd15))

```

## Tests of Suitability for Factor Analysis

### Pre-imputation

#### Bartlett's Test

This is a test that the matrix is an identity matrix. This would mean
that the correlations were not significantly different from 0.
If it's not significant, the matrix is not suitable because 
the variables show too little covariance.  

The chi-squared for the Bartlett test is 
`r preBartlett15$chisq` with
`r preBartlett15$df` DF, p = 
`r format(preBartlett15$p.value, scientific=TRUE)`.

#### Kaiser-Meyer-Olkin Criterion (KMO)

From EFAtools::KMO documentation:

> The KMO represents the degree to which each observed variable is predicted by the other variables in the dataset and with this indicates the suitability for factor analysis  

The numeric version of the dataset is used because stats::cor is used to find
the correlation and it requires numeric input. Used option for 
Spearman correlation because of ordered variables.  


``` {r, preImpKMO, results="asis", echo=FALSE}
KMOoutput15 <- KMO(ddNum15, cor_method = "spearman")
KMOoutput15$KMO
```

### Post-imputation

#### Bartlett's Test

``` {r, postImputationCorr, results="asis", echo=FALSE}
### generate polychoric correlation matrix
postImputedCor15 <- polychoric(imputedNumericDF15,
                               smooth = TRUE,
                               correct = 0.01)
postImputedCor15 <- round(postImputedCor15$rho, 3)
write.csv(postImputedCor15,
          file="data/3.0/postImputedCor15.csv",
          quote = FALSE,
          row.names=FALSE)

### Bartlett Test
postBartlett15 <- cortest.bartlett(postImputedCor15,
                                   n = nrow(ddOrd15))

```

This is a test that the matrix is an identity matrix. This would mean
that the correlations were not significantly different from 0.
If it's not significant, the matrix is not suitable because 
the variables show too little covariance.  
  


The chi-squared for the Bartlett test is 
`r postBartlett15$chisq` with
`r postBartlett15$df` DF, p = 
`r format(postBartlett15$p.value, scientific = TRUE)`.



#### Kaiser-Meyer-Olkin Criterion (KMO)

From EFAtools::KMO documentation:

> The KMO represents the degree to which each observed variable is predicted by the other variables in the dataset and with this indicates the suitability for factor analysis  

The numeric version of the dataset is used because stats::cor is used to find
the correlation and it requires numeric input. Used option for 
Spearman correlation because of ordered variables.  


``` {r, KMOpostImputed, results="asis", echo=FALSE}
KMOutput15 <- KMO(ddNum15, cor_method = "spearman")
KMOoutput15$KMO

```


## Between-Item Correlations

### Pre-imputation  

For factor analysis, it is recommended that some of the item correlationsshould be 
between 0.3 and 0.9.
Polychoric correlations were computed using the `polychoric` function in the `psych` package in R 
with options `smooth = TRUE` to adjust for non-positive-definite matrices 
and `correct = 0.01` to avoid computation problems with covariance of zero.


The minimum correlation in this data set is 
`r min(lowerTriangle(preImputedCor15))`.
The maximum correlation in this data set is 
`r max(lowerTriangle(preImputedCor15))`.

``` {r, preImputedCorPlot, results="asis", echo=FALSE}
cor.plot(preImputedCor15)
```

### Post-imputation

The post imputation polyserial correlation was also computed using the `polychoric`
function, with no setting `smooth = TRUE` and `correct = 0.01.`  

The minimum correlation was 
`r min(lowerTriangle(postImputedCor15))`.
The maximum correlation was 
`r max(lowerTriangle(postImputedCor15))`.

``` {r, postImputedCorrelation, results="asis", echo=FALSE}
cor.plot(postImputedCor15)
```

## Factor Analysis for 15 Factor Model

``` {r, modelFit15, results="asis", echo=FALSE}

fanal15 <- fa(imputedNumericDF15,
      nfactors = 15,
      rotate = "oblimin",
      fm = "pa",
      smooth = TRUE,
      cor = "poly",
      correct = 0.01,
      max.iter = 100000)

save(fanal15, file = "data/3.0/fanal15.Rda")

```

Although the chi-square test of goodness of fit is sensitive to departures from normality like
the C-BARQ items, Hopper et al (2008) recommend always reporting it.  

- chi-square: `r fanal15$STATISTIC`  
- degrees of freedom: `r fanal15$dof`  
- P-value for chi-square = `r format(fanal15$PVAL, scientific = TRUE)`  

Tucker-Lewis Index of Factoring Reliability/Non-Norm Fit Index:
`r fanal15$TLI`.
Should be > 0.9; need reference)

## Communalities

``` {r, communalities, results="asis", echo=FALSE}
commTab <- kable(data.frame(fanal15$communality), digits = 2)
kable_styling(commTab, full_width = FALSE)
```

### How many communalities < 0.40?

THere are `r length(fanal15[fanal15$communality < 0.40])`
items with communality < 0.40.

``` {r, lowCommTab15, results = "asis", echo = FALSE}
lc15 <- data.frame(fanal15$communality )
lc15$item <- colnames(ddOrd15)
lc15 <- lc15[lc15[1] < 0.40, ]
lcTab15 <- kable(lc15, digits = 2)
kable_styling(lcTab15, full_width = FALSE)


```

## Loadings

``` {r, loadings, results = "asis", echo = FALSE}
loadMat <- matrix(fanal15$loadings, nrow = ncol(ddOrd15))
dimnames(loadMat) <- dimnames(fanal15$loadings)
loadingsTab <- kable(loadMat, digits = 2)
kable_styling(loadingsTab)

### calculate max loading for each item
loadDF <- data.frame(loadMat)
loadDF$maximum <- apply(loadDF, 1, max)
loadDF$largest <- colnames(loadDF)[apply(loadDF, 1, which.max)]

### sort df by factor
loadDF <- loadDF[order(loadDF$largest), ]



```
						   
### Largest Loading per Item

``` {r, loadWithDesc, results="asis", echo=FALSE}
## merge description with loadings
loadDF$itemNames <- rownames(loadDF)
m1 <- merge(loadDF, descItems,
            by="itemNames",
            all.x = TRUE,
            all.y = FALSE)

### sort by factor
m1$fnum <- parse_number(m1$largest)
m1 <- m1[order(m1$fnum), ]



largestLoadings <- m1[ ,c("fnum", "maximum", "itemNames", "itemDescriptions")]

commTab <- data.frame(fanal15$communality)
commTab$itemNames <- rownames(commTab)
colnames(commTab) <- c("Communality", "itemNames")

mc1 <- merge(commTab, largestLoadings, by = "itemNames")
mc1$Communality <- round(mc1$Communality, 2)
mc1$maximum <- round(mc1$maximum, 2)
mc1 <- mc1[ ,c("fnum", "itemNames", "Communality", "maximum", "itemDescriptions")]
mc1 <- mc1[order(mc1$fnum), ]
colnames(mc1) <- c("Factor", "Item", "Communality", "Maximum Loading", "Description")
mc1K <- kable(mc1, digits = 2,
                         caption = "Communality, Largest Loading and factor sorted  by Factor")
kable_styling(mc1K, full_width = FALSE)

write_xlsx(mc1, "TableS7.xlsx")


```

## Factor Statistics

``` {r, perFactorStats, results="asis", echo=FALSE}
### we want to show eigenvalues in this table too
### eigenvalues are sum of squared loadings in fanal15$Vaccoutned
edf <- t(fanal15$Vaccounted)
edf <- data.frame(edf)
edf$f <- rownames(edf)
edf$fnum <- parse_number(edf$f)
names(edf)

edf <- edf[ ,c("fnum", "SS.loadings", "Proportion.Var", "Cumulative.Var", "Proportion.Explained","Cumulative.Proportion")]

colnames(edf) <- c("Factor",
                   "Eigenvalue (Sum-of-Squares Loadings",
                   "Proportion of Variance",
                   "Cumulative Proportion of Variance",
                   "Proportion Explained Variance",
                   "Cumulative Proportion of Explained Variance")

edfK <- kable(edf, digits = 2,
              caption = "Eigenvalues (Sums-of-squared loadings), cumulative variance, proportion of variance, proportion explained, and cumulative proportion for 15 factor model")
kable_styling(edfK, full_width = FALSE)

edfR <- edf %>%
    mutate(across(where(is.numeric), round, 2))
write_xlsx(edfR, "Table5.xlsx")

```

## Reliability

``` {r, reliability, results="asis", echo=FALSE}
keys15 <- factor2cluster(fanal15)
keysList15 <- keys2list(keys15)
scores15 <- scoreItems(keysList15,
                       imputedNumericDF15)
save(scores15, file = "data/3.0/scores15.Rda")


alphas <- data.frame(t(scores15$alpha))
alphas$factor <- rownames(alphas)
alphas <- alphas[order(row.names(alphas)), ]
colnames(alphas) <- c("alpha", "factor")
alphaTab <- kable(alphas[ ,c("factor", "alpha")], digits = 3)
kable_styling(alphaTab, full_width = FALSE)
``` 

From `EFAtools::OMEGA` documentation:  

Omegas refer to the correlation between a factor and a unit-weighted composite score and thus the true score variance in a unit-weighted composite based on the respective indicators. Omega total is the total true score variance in a composite. Omega hierarchical is the true score variance in a composite that is attributable to the general factor, and omega subscale is the true score variance in a composite attributable to all subscales / group factors (for the whole scale) or to the specific subscale / group factor (for subscale composites).


Interpretation of omega_subscale  

no universal guideline 

0.50 is minimum, 0.75 preferable
(watkins books,rodrigues16, zimbarg2005

``` {r, omegas, results="asis", echo=FALSE}
sl15 <- SL(fanal15, method = "PAF", type = "psych")
om15 <- OMEGA(sl15, type = "psych")
ns <- dimnames(om15)[[1]]
k <- matrix(om15, ncol = 6)
colnames(k) <- c("Total Variation", "Hierarchical Variation", "Subscale Variation", "H", "ECV", "PUC")
rownames(k) <- ns
omTab <- kable(k, digits = 2, row.names = TRUE)
kable_styling(omTab, full_width = FALSE)
    


om15

```

``` {r, ManuscriptReliabilityTab, results="asis", echo=FALSE}
### separate factor number from alphas table, factor var
alphas$fnum <- parse_number(alphas$factor)
alphas <- alphas[order(alphas$fnum), ]
### add top blank row because omega has the g row


### get omega table
omTemp <- matrix(om15, ncol = 6)
omTemp <- round(omTemp, 2)
omTemp <- data.frame(omTemp)
colnames(omTemp) <- c("TotalOmega", "HierarchicalOmega", "SubscleOmega", "H", "ECV", "PUC")
k <- dimnames(om15)[1]
k <- unlist(k)
rownames(omTemp) <- k
omTemp$factor <- rownames(omTemp)
omTemp$fnum <- c(NA, seq(1:15))

### merge alpha and omega
rel <- merge(alphas, omTemp, by="fnum", all=TRUE)

### sort numaric
rel <- rel[order(rel$fnum), ]

### bring bottom for the g factor to the top
rel <- rel[c(16, 1:15), ] 

### round(alpha)
rel$alpha <- round(rel$alpha, 2)

### keep only needed vars
rel <- rel[ ,c("factor.y", "alpha", "TotalOmega", "HierarchicalOmega", "SubscleOmega")]

### neaten colnames (put in spaces and remove .y) for output table
colnames(rel) <- c("Factor", "alpha", "Total Omega", "Hierarchical Omega", "Subscale Omega")
rel$Factor <- parse_number(rel$Factor)

write_xlsx(rel, "Table6.xlsx")

```

## Scores

``` {r, scoreTable, results="asis", echo=FALSE}
st <- scores15$scores
st <- data.frame(st)
longScores <- pivot_longer(st, everything(), names_to = "Factor", values_to = "Score")
scoreTab <- longScores %>%
    group_by(Factor) %>%
    dplyr::summarize(Mean = mean(Score),
                     SD = sd(Score)) %>%
mutate(across(where(is.numeric), round, 2))

scoreTab$Factor <- parse_number(scoreTab$Factor)
scoreTab <- scoreTab[order(scoreTab$Factor), ]

scoreTabK <- kable(scoreTab, digits=2,
                   caption = "Mean, Standard Deviation, and Median for Each of 15 Factors")
kable_styling(scoreTabK, full_width = FALSE)

write_xlsx(scoreTab, "Table7.xlsx")


```
